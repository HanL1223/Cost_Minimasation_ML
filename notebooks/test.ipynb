{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overarching importation\n",
    "import sys, os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # go up one directory from notebooks/\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9328fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Data_ingestor import DataIngestorFactory\n",
    "from src.Missing_value_handling import MissingValueHandler,FillMissingValue,DropMissingValue\n",
    "\n",
    "#-------#\n",
    "# To help with reading and manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# To help with data visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# To be used for missing value imputation\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# To help with model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# To get different metric scores, and split data\n",
    "from sklearn import metrics\n",
    "#---#\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# To oversample and undersample data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# To be used for data scaling and one hot encoding\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# To be used for tuning the model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# To use statistical functions\n",
    "import scipy.stats as stats\n",
    "\n",
    "# To be used for creating pipelines and personalizing them\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# To define maximum number of columns to be displayed in a dataframe\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# To supress scientific notations for a dataframe\n",
    "pd.set_option(\"display.float_format\", lambda x: \"%.3f\" % x)\n",
    "\n",
    "# To supress warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e764bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "TRAIN_ENDPOINT = f\"{BASE_URL}/train\"\n",
    "TEST_ENDPOINT = f\"{BASE_URL}/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08bae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "TRAIN_ENDPOINT = f\"{BASE_URL}/train\"\n",
    "TEST_ENDPOINT = f\"{BASE_URL}/test\"\n",
    "df_train = DataIngestorFactory.get_data_ingestor(TRAIN_ENDPOINT).ingest(TRAIN_ENDPOINT)\n",
    "df_test = DataIngestorFactory.get_data_ingestor(TEST_ENDPOINT).ingest(TEST_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = MissingValueHandler(FillMissingValue,method = 'mean')\n",
    "\n",
    "handler.handle_missing_values(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38276274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Data_Samplier import SamplerFactory\n",
    "\n",
    "samplier = SamplerFactory.create('smoteenn')\n",
    "df_scaled = samplier.impute(df_train,target_col= 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d93b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to scale up Trainig Set\n",
    "from src.Data_Samplier import SMOTEENNSampler\n",
    "df_scaled = SMOTEENNSampler().impute(df_train,target_col='Target')\n",
    "df_scaled['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b6478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Model_Selector import CrossValidationEvaluation,ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e182ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_scaled.drop('Target', axis=1)\n",
    "y = df_scaled['Target']\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,     \n",
    "    random_state=42,    \n",
    "    stratify=y           \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Model Selection\n",
    "models = [\n",
    "    (\"log\", LogisticRegression(solver=\"newton-cg\", random_state=42)),\n",
    "    (\"DecisionTreeClassifier\", DecisionTreeClassifier(random_state=42)),\n",
    "    (\"XGBClassifier\", XGBClassifier(random_state=42, eval_metric=\"logloss\", device='cpu'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff789e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create evaluation strategy\n",
    "strategy = CrossValidationEvaluation(n_splits=5, random_state=42)\n",
    "\n",
    "# Step 2: create the evaluator with that strategy\n",
    "evaluator = ModelEvaluator(strategy=strategy)\n",
    "\n",
    "# Step 3: evaluate  models\n",
    "results = evaluator.evaluate_models(models=models, X=X_train, y=y_train)\n",
    "\n",
    "# Step 4: Return best model name and best model attribute\n",
    "best_model_name,best_model = evaluator.get_best_model(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cec450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Tunning\n",
    "from src.Model_Tuner import OptunaTuning\n",
    "\n",
    "#Step 1 Define Tuning configuration\n",
    "tuner = OptunaTuning(config_dir=\"config\", n_trials=50, cv_folds=5)\n",
    "\n",
    "#Step 2 Run Tuning to get final model, parameter and best score\n",
    "best_model, best_params, best_score = tuner.tune(\"xgbclassifier\", XGBClassifier, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6c67f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.save_tuned_model(\"xgbclassifier\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5072ce30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Model_Evaluator import ClassificationModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e2a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_map = {\n",
    "        \"classification\": '123'\n",
    "       # \"regression\": \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb89a0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"http://127.0.0.1:8000\"\n",
    "TRAIN_ENDPOINT = f\"{BASE_URL}/train\"\n",
    "TEST_ENDPOINT = f\"{BASE_URL}/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bc4ad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overarching importation\n",
    "import sys, os\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # go up one directory from notebooks/\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "os.chdir(PROJECT_ROOT)\n",
    "from zenml import pipeline\n",
    "from steps.Data_Ingestion_Steps import data_ingestion_step\n",
    "from steps.Data_Split_Steps import data_split_step\n",
    "from steps.Data_Sampling_Steps import data_sampling_step\n",
    "from steps.Model_Selection_Steps import model_selection_step\n",
    "from steps.Model_Tuning_Steps import model_tuning_step\n",
    "from steps.Missing_data_Handling_Steps import missing_value_step\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from src.Get_Logging_Config import get_logger\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12a1ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37b538c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 21:26:06 [INFO] __main__: Starting data ingestion\n",
      "\u001b[1;35mStarting data ingestion\u001b[0m\n",
      "\u001b[1;35mRunning single step pipeline to execute step \u001b[0m\u001b[1;36mdata_ingestion_step\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mdata_ingestion_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[33mIn a future release, the default Python package installer used by ZenML to build container images for your containerized pipelines will change from 'pip' to 'uv'. To maintain current behavior, you can explicitly set \u001b[0m\u001b[1;36mpython_package_installer=PythonPackageInstaller.PIP\u001b[33m in your DockerSettings.\u001b[0m\n",
      "\u001b[1;35mCaching is disabled by default for \u001b[0m\u001b[1;36mdata_ingestion_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml login --local\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mdata_ingestion_step\u001b[1;35m has started.\u001b[0m\n",
      "[data_ingestion_step] 2025-10-22 21:26:09 [INFO] src.Data_ingestor: Fetching data from API endpoint: http://127.0.0.1:8000/train\n",
      "[data_ingestion_step] \u001b[1;35mFetching data from API endpoint: \u001b[0m\u001b[34mhttp://127.0.0.1:8000/train\u001b[1;35m\u001b[0m\n",
      "[data_ingestion_step] 2025-10-22 21:26:11 [INFO] src.Data_ingestor: Received 40000 records from API\n",
      "[data_ingestion_step] \u001b[1;35mReceived 40000 records from API\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mdata_ingestion_step\u001b[1;35m has finished in \u001b[0m\u001b[1;36m4.781s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m5.024s\u001b[1;35m.\u001b[0m\n",
      "2025-10-22 21:26:15 [INFO] __main__: Data ingestion completed: (40000, 41)\n",
      "\u001b[1;35mData ingestion completed: (40000, 41)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Data Ingestion Step\n",
    "logger.info(\"Starting data ingestion\")\n",
    "raw_data = data_ingestion_step(TRAIN_ENDPOINT)\n",
    "logger.info(f\"Data ingestion completed: {raw_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee297040",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc06bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 21:56:09 [INFO] __main__:  Handling missing data...\n",
      "\u001b[1;35m Handling missing data...\u001b[0m\n",
      "2025-10-22 21:56:09 [INFO] src.Missing_value_handling: Executing missing value handling strategy.\n",
      "\u001b[1;35mExecuting missing value handling strategy.\u001b[0m\n",
      "2025-10-22 21:56:09 [INFO] src.Missing_value_handling: Filling value with mode strategy\n",
      "\u001b[1;35mFilling value with mode strategy\u001b[0m\n",
      "2025-10-22 21:56:10 [INFO] src.Missing_value_handling: Missing values filled.\n",
      "\u001b[1;35mMissing values filled.\u001b[0m\n",
      "2025-10-22 21:56:10 [INFO] __main__: Missing data handled: (40000, 41)\n",
      "\u001b[1;35mMissing data handled: (40000, 41)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: Missing Data Handling ===\n",
    "logger.info(\" Handling missing data...\")\n",
    "clean_data = missing_value_step(raw_data,strategy='fill',method = 'mode')\n",
    "logger.info(f\"Missing data handled: {clean_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f076cf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 22:02:21 [INFO] __main__: Performing data sampling...\n",
      "\u001b[1;35mPerforming data sampling...\u001b[0m\n",
      "2025-10-22 22:02:21 [INFO] src.Data_Samplier: Applying SMOTEENN hybrid resampling. Input size: (40000, 41)\n",
      "\u001b[1;35mApplying SMOTEENN hybrid resampling. Input size: (40000, 41)\u001b[0m\n",
      "2025-10-22 22:02:26 [INFO] src.Data_Samplier: SMOTEENN complete. Output size: (73091, 41)\n",
      "\u001b[1;35mSMOTEENN complete. Output size: (73091, 41)\u001b[0m\n",
      "2025-10-22 22:02:26 [INFO] src.Data_Samplier: y Class: Target\n",
      "1.0    37613\n",
      "0.0    35478\n",
      "Name: count, dtype: int64\n",
      "\u001b[1;35my Class: Target\n",
      "1.0    37613\n",
      "0.0    35478\n",
      "Name: count, dtype: int64\u001b[0m\n",
      "2025-10-22 22:02:26 [INFO] steps.Data_Sampling_Steps: Applying sampling. using smoteenn\n",
      "\u001b[1;35mApplying sampling. using smoteenn\u001b[0m\n",
      "2025-10-22 22:02:26 [INFO] __main__: Sampling completed: (73091, 41)\n",
      "\u001b[1;35mSampling completed: (73091, 41)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# === Step 3: Sampling (if needed) ===\n",
    "logger.info(\"Performing data sampling...\")\n",
    "sampled_data = data_sampling_step(method='smoteenn',df = clean_data,target_col='Target')\n",
    "logger.info(f\"Sampling completed: {sampled_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d961c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 22:17:00 [INFO] __main__:  Splitting data into train/test sets...\n",
      "\u001b[1;35m Splitting data into train/test sets...\u001b[0m\n",
      "\u001b[1;35mRunning single step pipeline to execute step \u001b[0m\u001b[1;36mdata_split_step\u001b[1;35m\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mdata_split_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_5514d922-a640-40a3-8bc9-1276465e2f8a'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact 1cce898e-53a7-4417-9706-b764695aaba2.\u001b[0m\n",
      "\u001b[1;35mCaching is disabled by default for \u001b[0m\u001b[1;36mdata_split_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml login --local\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mdata_split_step\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mdata_split_step\u001b[1;35m has finished in \u001b[0m\u001b[1;36m3.495s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m3.733s\u001b[1;35m.\u001b[0m\n",
      "2025-10-22 22:17:07 [INFO] __main__: Split done: Train=58472, Test=14619\n",
      "\u001b[1;35mSplit done: Train=58472, Test=14619\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Data Split ===\n",
    "logger.info(\" Splitting data into train/test sets...\")\n",
    "X_train, X_test, y_train, y_test = data_split_step(df = sampled_data,target_col = 'Target',test_size = 0.2)\n",
    "logger.info(f\"Split done: Train={len(X_train)}, Test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "868ec30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 23:01:47 [INFO] __main__: 🤖 Selecting the best model...\n",
      "\u001b[1;35m🤖 Selecting the best model...\u001b[0m\n",
      "\u001b[1;35mRunning single step pipeline to execute step \u001b[0m\u001b[1;36mmodel_selection_step\u001b[1;35m\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mmodel_selection_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_98ae98ec-14dc-4bd1-a972-85cc5d3ede52'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact ec44fa61-d82b-4364-bf34-ff0c11af3fcc.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_2c40aada-2f50-48e3-a1c0-e4ed86484527'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact eba28859-f59e-44f5-b08c-b7b4400b7c38.\u001b[0m\n",
      "\u001b[1;35mCaching is disabled by default for \u001b[0m\u001b[1;36mmodel_selection_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml login --local\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_selection_step\u001b[1;35m has started.\u001b[0m\n",
      "[model_selection_step] 2025-10-22 23:01:49 [INFO] src.Model_Selector: Evaluating models using selected strategy\n",
      "[model_selection_step] \u001b[1;35mEvaluating models using selected strategy\u001b[0m\n",
      "[model_selection_step] 2025-10-22 23:01:49 [INFO] src.Model_Selector: Starting cross-validation with cost-sensitive scoring\n",
      "[model_selection_step] \u001b[1;35mStarting cross-validation with cost-sensitive scoring\u001b[0m\n",
      "[model_selection_step] 2025-10-22 23:01:52 [INFO] src.Model_Selector: log: Mean cost ratio = 0.8057 (±0.0055)\n",
      "[model_selection_step] \u001b[1;35mlog: Mean cost ratio = 0.8057 (±0.0055)\u001b[0m\n",
      "[model_selection_step] 2025-10-22 23:02:06 [INFO] src.Model_Selector: DecisionTreeClassifier: Mean cost ratio = 0.9421 (±0.0023)\n",
      "[model_selection_step] \u001b[1;35mDecisionTreeClassifier: Mean cost ratio = 0.9421 (±0.0023)\u001b[0m\n",
      "[model_selection_step] 2025-10-22 23:02:09 [INFO] src.Model_Selector: XGBClassifier: Mean cost ratio = 0.9747 (±0.0018)\n",
      "[model_selection_step] \u001b[1;35mXGBClassifier: Mean cost ratio = 0.9747 (±0.0018)\u001b[0m\n",
      "[model_selection_step] 2025-10-22 23:02:09 [INFO] src.Model_Selector: Best model: XGBClassifier (Score: 0.9747)\n",
      "[model_selection_step] \u001b[1;35mBest model: XGBClassifier (Score: 0.9747)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_selection_step\u001b[1;35m has finished in \u001b[0m\u001b[1;36m20.683s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m20.927s\u001b[1;35m.\u001b[0m\n",
      "2025-10-22 23:02:11 [INFO] __main__: Best model: XGBClassifier\n",
      "\u001b[1;35mBest model: XGBClassifier\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# === Step 5: Model Selection ===\n",
    "logger.info(\"🤖 Selecting the best model...\")\n",
    "best_model_name, best_model = model_selection_step(X_train,y_train)\n",
    "logger.info(f\"Best model: {best_model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Tunning\n",
    "from src.Model_Tuner import OptunaTuning\n",
    "\n",
    "#Step 1 Define Tuning configuration\n",
    "tuner = OptunaTuning(config_dir=\"config\", n_trials=50, cv_folds=5)\n",
    "\n",
    "#Step 2 Run Tuning to get final model, parameter and best score\n",
    "best_model, best_params, best_score = tuner.tune(\"xgbclassifier\", XGBClassifier, X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 23:17:03 [INFO] __main__: Tuning best model...\n",
      "\u001b[1;35mTuning best model...\u001b[0m\n",
      "\u001b[1;35mRunning single step pipeline to execute step \u001b[0m\u001b[1;36mmodel_tuning_step\u001b[1;35m\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[33mUsing an external artifact as step input currently invalidates caching for the step and all downstream steps. Future releases will introduce hashing of artifacts which will improve this behavior.\u001b[0m\n",
      "\u001b[1;35mInitiating a new run for the pipeline: \u001b[0m\u001b[1;36mmodel_tuning_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_df8fb76f-4b93-40fb-bf32-a8a42df43d7a'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact dade3a97-34e5-489b-a8be-bdca707fea09.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_59a2e0b2-f9db-4a35-81a9-90df152f602c'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact b2461945-2629-47b6-9385-a84455781ae8.\u001b[0m\n",
      "\u001b[1;35mUploading external artifact to 'external_artifacts/external_b6961c49-ef8d-45ee-92c1-c11eb9a59833'.\u001b[0m\n",
      "\u001b[1;35mFinished uploading external artifact 625a2c18-d850-47c5-85e5-ea770b51e298.\u001b[0m\n",
      "\u001b[1;35mCaching is disabled by default for \u001b[0m\u001b[1;36mmodel_tuning_step\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mUsing user: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mUsing stack: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  artifact_store: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35m  orchestrator: \u001b[0m\u001b[1;36mdefault\u001b[1;35m\u001b[0m\n",
      "\u001b[1;35mYou can visualize your pipeline runs in the \u001b[0m\u001b[1;36mZenML Dashboard\u001b[1;35m. In order to try it locally, please run \u001b[0m\u001b[1;36mzenml login --local\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_tuning_step\u001b[1;35m has started.\u001b[0m\n",
      "[model_tuning_step] 2025-10-22 23:17:06 [INFO] src.Model_Tuner: Loaded tuning configuration for xgbclassifier from config/xgbclassifier.yaml\n",
      "[model_tuning_step] \u001b[1;35mLoaded tuning configuration for xgbclassifier from config/xgbclassifier.yaml\u001b[0m\n",
      "[model_tuning_step] 2025-10-22 23:17:06 [INFO] src.Model_Tuner: Starting Optuna optimization for xgbclassifier (50 trials)...\n",
      "[model_tuning_step] \u001b[1;35mStarting Optuna optimization for xgbclassifier (50 trials)...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[model_tuning_step] [I 2025-10-22 23:17:06,295] A new study created in memory with name: xgbclassifier_optuna_tuning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83089b482ba441c9835c222f877c3899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model_tuning_step] [I 2025-10-22 23:17:11,984] Trial 0 finished with value: 0.984499892403494 and parameters: {'max_depth': 10, 'learning_rate': 0.1745205335405329, 'n_estimators': 1425, 'subsample': 0.6188338544440873, 'colsample_bytree': 0.9549710323520944, 'gamma': 0.18462668954740136, 'min_child_weight': 8, 'reg_alpha': 0.5411819568860157, 'reg_lambda': 0.234058363334949}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:13,868] Trial 1 finished with value: 0.9585875724452763 and parameters: {'max_depth': 3, 'learning_rate': 0.29087979044703577, 'n_estimators': 448, 'subsample': 0.9941911803922239, 'colsample_bytree': 0.7841158667666361, 'gamma': 0.17559777740408233, 'min_child_weight': 4, 'reg_alpha': 0.9232564061243445, 'reg_lambda': 0.9255221629903605}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:17,664] Trial 2 finished with value: 0.9583598901523706 and parameters: {'max_depth': 11, 'learning_rate': 0.13878600763737528, 'n_estimators': 1103, 'subsample': 0.6712422139955836, 'colsample_bytree': 0.55469773424428, 'gamma': 2.73518482665464, 'min_child_weight': 3, 'reg_alpha': 0.7989824813445714, 'reg_lambda': 0.4212994292388398}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:21,252] Trial 3 finished with value: 0.9575136531615233 and parameters: {'max_depth': 4, 'learning_rate': 0.29077839684865164, 'n_estimators': 1209, 'subsample': 0.6985160755023541, 'colsample_bytree': 0.7381313195394577, 'gamma': 3.0382517678582173, 'min_child_weight': 2, 'reg_alpha': 0.2675748755306756, 'reg_lambda': 0.5646297622766491}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:25,381] Trial 4 finished with value: 0.959105408956 and parameters: {'max_depth': 12, 'learning_rate': 0.02894500167858163, 'n_estimators': 380, 'subsample': 0.6704066242278965, 'colsample_bytree': 0.5176706931260746, 'gamma': 2.388990417379335, 'min_child_weight': 5, 'reg_alpha': 0.6364021017728504, 'reg_lambda': 0.2526514277479617}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:26,667] Trial 5 finished with value: 0.9696696134676737 and parameters: {'max_depth': 6, 'learning_rate': 0.17798111165236397, 'n_estimators': 165, 'subsample': 0.7723812391649825, 'colsample_bytree': 0.5884274067021695, 'gamma': 1.0665913767141932, 'min_child_weight': 1, 'reg_alpha': 0.6303488740891765, 'reg_lambda': 0.19073114797801438}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:28,333] Trial 6 finished with value: 0.9158826086057218 and parameters: {'max_depth': 5, 'learning_rate': 0.06203879409984791, 'n_estimators': 244, 'subsample': 0.5244302444566062, 'colsample_bytree': 0.9820949033237819, 'gamma': 4.997036744435119, 'min_child_weight': 2, 'reg_alpha': 0.6547474948570132, 'reg_lambda': 0.2913882948992732}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:33,047] Trial 7 finished with value: 0.9105192832780938 and parameters: {'max_depth': 4, 'learning_rate': 0.060803239721153216, 'n_estimators': 1866, 'subsample': 0.9782606333903066, 'colsample_bytree': 0.7091997190725661, 'gamma': 3.766742223129143, 'min_child_weight': 8, 'reg_alpha': 0.8603112967846043, 'reg_lambda': 0.37453253331112346}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:37,506] Trial 8 finished with value: 0.9408887367443025 and parameters: {'max_depth': 4, 'learning_rate': 0.07801881273773191, 'n_estimators': 1516, 'subsample': 0.8134830837529995, 'colsample_bytree': 0.8784845092316048, 'gamma': 2.5839026959430007, 'min_child_weight': 8, 'reg_alpha': 0.9299810830147955, 'reg_lambda': 0.34491037915694656}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:40,120] Trial 9 finished with value: 0.9285751138399517 and parameters: {'max_depth': 4, 'learning_rate': 0.08357823904436966, 'n_estimators': 826, 'subsample': 0.807330868705951, 'colsample_bytree': 0.6599623435450228, 'gamma': 3.454372784329378, 'min_child_weight': 4, 'reg_alpha': 0.5988616266469698, 'reg_lambda': 0.7226238893307647}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:45,852] Trial 10 finished with value: 0.9770965300430031 and parameters: {'max_depth': 9, 'learning_rate': 0.20958193061289615, 'n_estimators': 1934, 'subsample': 0.5064656289680391, 'colsample_bytree': 0.9938403579715364, 'gamma': 1.1552664220751554, 'min_child_weight': 10, 'reg_alpha': 0.24064949870699415, 'reg_lambda': 0.015193037768268691}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:51,529] Trial 11 finished with value: 0.9770208646608968 and parameters: {'max_depth': 9, 'learning_rate': 0.22507810497951142, 'n_estimators': 1943, 'subsample': 0.5156762616997083, 'colsample_bytree': 0.9991281993781463, 'gamma': 1.0447000138482208, 'min_child_weight': 10, 'reg_alpha': 0.312067082190811, 'reg_lambda': 0.005163619560271782}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:17:59,632] Trial 12 finished with value: 0.9836529033667656 and parameters: {'max_depth': 9, 'learning_rate': 0.22055526632607847, 'n_estimators': 1539, 'subsample': 0.591454616056504, 'colsample_bytree': 0.8927777849940465, 'gamma': 0.04973746503977669, 'min_child_weight': 10, 'reg_alpha': 0.022491103702428017, 'reg_lambda': 0.014583675881889913}. Best is trial 0 with value: 0.984499892403494.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:08,609] Trial 13 finished with value: 0.9852833016102794 and parameters: {'max_depth': 9, 'learning_rate': 0.1406991095107773, 'n_estimators': 1482, 'subsample': 0.5830393755037669, 'colsample_bytree': 0.8794092392576585, 'gamma': 0.0783766282157532, 'min_child_weight': 8, 'reg_alpha': 0.05976937535768906, 'reg_lambda': 0.12385917002877517}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:12,873] Trial 14 finished with value: 0.9695325961337261 and parameters: {'max_depth': 7, 'learning_rate': 0.13143210926338772, 'n_estimators': 1413, 'subsample': 0.5992297385678033, 'colsample_bytree': 0.868921097367642, 'gamma': 1.7673572797274362, 'min_child_weight': 7, 'reg_alpha': 0.4630886572959525, 'reg_lambda': 0.15779699023708108}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:16,067] Trial 15 finished with value: 0.9809228662278098 and parameters: {'max_depth': 10, 'learning_rate': 0.17568819179763492, 'n_estimators': 792, 'subsample': 0.6103026135953646, 'colsample_bytree': 0.8102699397553055, 'gamma': 0.6394129558859035, 'min_child_weight': 7, 'reg_alpha': 0.013476960044184902, 'reg_lambda': 0.565693049356426}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:20,550] Trial 16 finished with value: 0.966716314391465 and parameters: {'max_depth': 8, 'learning_rate': 0.11571672198427632, 'n_estimators': 1672, 'subsample': 0.8644718070049218, 'colsample_bytree': 0.9137587948835517, 'gamma': 1.6543049228280868, 'min_child_weight': 8, 'reg_alpha': 0.44054169287540423, 'reg_lambda': 0.1369246345571036}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:25,237] Trial 17 finished with value: 0.9825435542062262 and parameters: {'max_depth': 12, 'learning_rate': 0.16896181741930585, 'n_estimators': 1309, 'subsample': 0.5795737281166238, 'colsample_bytree': 0.9358223678789686, 'gamma': 0.4745615812431657, 'min_child_weight': 6, 'reg_alpha': 0.37882737918574694, 'reg_lambda': 0.4914242417562515}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:28,071] Trial 18 finished with value: 0.9683254782447379 and parameters: {'max_depth': 10, 'learning_rate': 0.2579790866488091, 'n_estimators': 957, 'subsample': 0.7334091944599943, 'colsample_bytree': 0.8261872286243193, 'gamma': 1.8145918032663184, 'min_child_weight': 9, 'reg_alpha': 0.18780044328193324, 'reg_lambda': 0.12579283708343247}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:33,954] Trial 19 finished with value: 0.9807626003935915 and parameters: {'max_depth': 7, 'learning_rate': 0.11269042731976285, 'n_estimators': 1728, 'subsample': 0.6311678666598863, 'colsample_bytree': 0.9410106156605664, 'gamma': 0.643366411615075, 'min_child_weight': 6, 'reg_alpha': 0.1278016395611328, 'reg_lambda': 0.9561866324130079}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:36,225] Trial 20 finished with value: 0.9436446915461877 and parameters: {'max_depth': 11, 'learning_rate': 0.20385344029703548, 'n_estimators': 672, 'subsample': 0.5400645934338327, 'colsample_bytree': 0.8419325239446349, 'gamma': 4.69182975765135, 'min_child_weight': 9, 'reg_alpha': 0.7652723924193945, 'reg_lambda': 0.732259097184969}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:44,175] Trial 21 finished with value: 0.9834719712732685 and parameters: {'max_depth': 9, 'learning_rate': 0.2392972733550387, 'n_estimators': 1532, 'subsample': 0.5599226605982401, 'colsample_bytree': 0.8907542220010943, 'gamma': 0.07968008648945629, 'min_child_weight': 9, 'reg_alpha': 0.0024722999106952803, 'reg_lambda': 0.0647634278215918}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:52,734] Trial 22 finished with value: 0.984491129730281 and parameters: {'max_depth': 8, 'learning_rate': 0.1906318031412839, 'n_estimators': 1676, 'subsample': 0.6441573394992207, 'colsample_bytree': 0.940477183859945, 'gamma': 0.012700039851490472, 'min_child_weight': 10, 'reg_alpha': 0.12648112358084446, 'reg_lambda': 0.23245376503824045}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:18:58,822] Trial 23 finished with value: 0.9816456644017725 and parameters: {'max_depth': 8, 'learning_rate': 0.15563230717857113, 'n_estimators': 1722, 'subsample': 0.6659609881170727, 'colsample_bytree': 0.9487346162711607, 'gamma': 0.5526640326855432, 'min_child_weight': 7, 'reg_alpha': 0.15282107628349012, 'reg_lambda': 0.23233659866904544}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:03,179] Trial 24 finished with value: 0.9757381706492531 and parameters: {'max_depth': 10, 'learning_rate': 0.18469807572750901, 'n_estimators': 1284, 'subsample': 0.6378778113061508, 'colsample_bytree': 0.9555623619504644, 'gamma': 1.4081004758628426, 'min_child_weight': 9, 'reg_alpha': 0.08640432898419367, 'reg_lambda': 0.29314621529168433}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:07,962] Trial 25 finished with value: 0.9780357411078107 and parameters: {'max_depth': 8, 'learning_rate': 0.14201418018904396, 'n_estimators': 1398, 'subsample': 0.6966677144530804, 'colsample_bytree': 0.850860052732834, 'gamma': 0.8107985242103719, 'min_child_weight': 8, 'reg_alpha': 0.34802660291719756, 'reg_lambda': 0.45073912485726697}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:12,446] Trial 26 finished with value: 0.9826178707951682 and parameters: {'max_depth': 11, 'learning_rate': 0.1989197632482041, 'n_estimators': 1116, 'subsample': 0.7380730057302785, 'colsample_bytree': 0.793325424799073, 'gamma': 0.3609501499994073, 'min_child_weight': 10, 'reg_alpha': 0.5433075436732204, 'reg_lambda': 0.07678017593025019}. Best is trial 13 with value: 0.9852833016102794.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:22,204] Trial 27 finished with value: 0.9853179585421088 and parameters: {'max_depth': 6, 'learning_rate': 0.10668683768472133, 'n_estimators': 1643, 'subsample': 0.5569886550050027, 'colsample_bytree': 0.9142236649417467, 'gamma': 0.040720841691732027, 'min_child_weight': 7, 'reg_alpha': 0.20532047349223703, 'reg_lambda': 0.2058159952567846}. Best is trial 27 with value: 0.9853179585421088.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:27,426] Trial 28 finished with value: 0.9662273906738426 and parameters: {'max_depth': 6, 'learning_rate': 0.10333753169643105, 'n_estimators': 1789, 'subsample': 0.5537401353060358, 'colsample_bytree': 0.9122514224800904, 'gamma': 2.1090500745448826, 'min_child_weight': 6, 'reg_alpha': 0.41659111893281286, 'reg_lambda': 0.3535376829341061}. Best is trial 27 with value: 0.9853179585421088.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:36,661] Trial 29 finished with value: 0.9356682116463764 and parameters: {'max_depth': 6, 'learning_rate': 0.01006765909127555, 'n_estimators': 1444, 'subsample': 0.5642808122809605, 'colsample_bytree': 0.7552469361711449, 'gamma': 0.39949414209649137, 'min_child_weight': 5, 'reg_alpha': 0.5278331574953532, 'reg_lambda': 0.10686338925668061}. Best is trial 27 with value: 0.9853179585421088.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:40,758] Trial 30 finished with value: 0.9682881610921867 and parameters: {'max_depth': 7, 'learning_rate': 0.1561878306267258, 'n_estimators': 1603, 'subsample': 0.893476006455761, 'colsample_bytree': 0.6639516753962234, 'gamma': 1.3738885328038164, 'min_child_weight': 7, 'reg_alpha': 0.22072207185435475, 'reg_lambda': 0.8080808872465277}. Best is trial 27 with value: 0.9853179585421088.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:49,529] Trial 31 finished with value: 0.9857134302806969 and parameters: {'max_depth': 8, 'learning_rate': 0.1311585671638698, 'n_estimators': 1636, 'subsample': 0.6329556767173284, 'colsample_bytree': 0.9650889366501909, 'gamma': 0.05242229446014293, 'min_child_weight': 8, 'reg_alpha': 0.09675251466272686, 'reg_lambda': 0.2488509824055381}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:19:55,185] Trial 32 finished with value: 0.9796057325837536 and parameters: {'max_depth': 10, 'learning_rate': 0.12432295314660458, 'n_estimators': 1809, 'subsample': 0.6131338958061291, 'colsample_bytree': 0.9745032676705783, 'gamma': 0.8631431748173715, 'min_child_weight': 8, 'reg_alpha': 0.07635470990532572, 'reg_lambda': 0.1927748979138412}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:00,942] Trial 33 finished with value: 0.983729586134379 and parameters: {'max_depth': 9, 'learning_rate': 0.09737770986161634, 'n_estimators': 1287, 'subsample': 0.6977182689328476, 'colsample_bytree': 0.9107445519853072, 'gamma': 0.27428686312097295, 'min_child_weight': 7, 'reg_alpha': 0.2752200779543996, 'reg_lambda': 0.3061183900675229}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:07,317] Trial 34 finished with value: 0.9840288657600501 and parameters: {'max_depth': 5, 'learning_rate': 0.14692547745712528, 'n_estimators': 1170, 'subsample': 0.5813448503931014, 'colsample_bytree': 0.9624652496166484, 'gamma': 0.023256020388163656, 'min_child_weight': 8, 'reg_alpha': 0.7407076031318864, 'reg_lambda': 0.39709719415851097}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:11,531] Trial 35 finished with value: 0.9836469584941847 and parameters: {'max_depth': 7, 'learning_rate': 0.16341414328187295, 'n_estimators': 1044, 'subsample': 0.6658867644655416, 'colsample_bytree': 0.85659823256675, 'gamma': 0.3333463786203649, 'min_child_weight': 6, 'reg_alpha': 0.06943107592712057, 'reg_lambda': 0.1831300225045633}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:16,796] Trial 36 finished with value: 0.9784076411455258 and parameters: {'max_depth': 11, 'learning_rate': 0.13442873781683085, 'n_estimators': 1594, 'subsample': 0.5006006448038434, 'colsample_bytree': 0.9154199478974607, 'gamma': 0.9206912365982455, 'min_child_weight': 9, 'reg_alpha': 0.21529343060423073, 'reg_lambda': 0.5528424375772802}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:21,678] Trial 37 finished with value: 0.9279872209999876 and parameters: {'max_depth': 3, 'learning_rate': 0.04607539891504611, 'n_estimators': 1425, 'subsample': 0.5361274940929812, 'colsample_bytree': 0.7490423393674823, 'gamma': 1.363180129293863, 'min_child_weight': 5, 'reg_alpha': 0.17408370196728348, 'reg_lambda': 0.26103688824603505}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:27,018] Trial 38 finished with value: 0.9552106124432524 and parameters: {'max_depth': 5, 'learning_rate': 0.09111969061019344, 'n_estimators': 1868, 'subsample': 0.6294243441671838, 'colsample_bytree': 0.9744707399860988, 'gamma': 3.0346257317234926, 'min_child_weight': 4, 'reg_alpha': 0.307873577390216, 'reg_lambda': 0.4395784452135386}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:32,865] Trial 39 finished with value: 0.9776469142731994 and parameters: {'max_depth': 6, 'learning_rate': 0.07484333342394323, 'n_estimators': 1622, 'subsample': 0.7038169880564893, 'colsample_bytree': 0.7774433647492094, 'gamma': 0.6710373087541611, 'min_child_weight': 7, 'reg_alpha': 0.7156714853140854, 'reg_lambda': 0.2000212681309464}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:36,744] Trial 40 finished with value: 0.9823706385755655 and parameters: {'max_depth': 8, 'learning_rate': 0.29317478117200646, 'n_estimators': 1254, 'subsample': 0.7754306015838484, 'colsample_bytree': 0.8838410253896496, 'gamma': 0.27638057405810706, 'min_child_weight': 8, 'reg_alpha': 0.5852914029867695, 'reg_lambda': 0.0677270220717763}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:45,141] Trial 41 finished with value: 0.9848225187661204 and parameters: {'max_depth': 8, 'learning_rate': 0.1884128606766109, 'n_estimators': 1730, 'subsample': 0.6480395590169946, 'colsample_bytree': 0.9349330714989867, 'gamma': 0.0471666772390242, 'min_child_weight': 9, 'reg_alpha': 0.11467451877239948, 'reg_lambda': 0.24859271722110993}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:20:52,501] Trial 42 finished with value: 0.9849934018715004 and parameters: {'max_depth': 9, 'learning_rate': 0.12230021801777975, 'n_estimators': 1992, 'subsample': 0.6466788642535742, 'colsample_bytree': 0.9311720175566245, 'gamma': 0.2691109928139661, 'min_child_weight': 9, 'reg_alpha': 0.09003955234989303, 'reg_lambda': 0.30442010331947167}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:02,823] Trial 43 finished with value: 0.9855721601735963 and parameters: {'max_depth': 9, 'learning_rate': 0.11590562434033722, 'n_estimators': 1972, 'subsample': 0.6501747818384395, 'colsample_bytree': 0.9271054958372613, 'gamma': 0.008007137207919506, 'min_child_weight': 9, 'reg_alpha': 0.09889290817419571, 'reg_lambda': 0.3080323146854137}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:09,954] Trial 44 finished with value: 0.9825515886739502 and parameters: {'max_depth': 9, 'learning_rate': 0.11278965257967648, 'n_estimators': 1945, 'subsample': 0.6092010870370889, 'colsample_bytree': 0.8981367214023717, 'gamma': 0.28922416185465416, 'min_child_weight': 8, 'reg_alpha': 0.9991868133163868, 'reg_lambda': 0.33331819109862537}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:14,878] Trial 45 finished with value: 0.9507690346379183 and parameters: {'max_depth': 9, 'learning_rate': 0.12517710256171163, 'n_estimators': 1973, 'subsample': 0.6826542829149023, 'colsample_bytree': 0.9983218105834939, 'gamma': 4.04354418075286, 'min_child_weight': 9, 'reg_alpha': 0.05819539451791362, 'reg_lambda': 0.2875310392755389}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:20,911] Trial 46 finished with value: 0.9735312251494627 and parameters: {'max_depth': 7, 'learning_rate': 0.06733161931838233, 'n_estimators': 1851, 'subsample': 0.7184075827005315, 'colsample_bytree': 0.9279180939804182, 'gamma': 1.1890425350012528, 'min_child_weight': 9, 'reg_alpha': 0.16565334896359968, 'reg_lambda': 0.4006742026441391}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:27,643] Trial 47 finished with value: 0.9802263311667361 and parameters: {'max_depth': 9, 'learning_rate': 0.10479738461185932, 'n_estimators': 1979, 'subsample': 0.5870953245158675, 'colsample_bytree': 0.8758432628877076, 'gamma': 0.7880445240655827, 'min_child_weight': 1, 'reg_alpha': 0.04569956625289053, 'reg_lambda': 0.32345753791861276}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:34,078] Trial 48 finished with value: 0.9807310746849595 and parameters: {'max_depth': 10, 'learning_rate': 0.09077650835736703, 'n_estimators': 1797, 'subsample': 0.7631836585850422, 'colsample_bytree': 0.558183146393883, 'gamma': 0.5139068756714369, 'min_child_weight': 8, 'reg_alpha': 0.24843266241995482, 'reg_lambda': 0.14105531813619118}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] [I 2025-10-22 23:21:41,204] Trial 49 finished with value: 0.9853695207456206 and parameters: {'max_depth': 8, 'learning_rate': 0.1436214702523277, 'n_estimators': 1913, 'subsample': 0.650628254507788, 'colsample_bytree': 0.8297626630457451, 'gamma': 0.20150198459714952, 'min_child_weight': 7, 'reg_alpha': 0.091668920605622, 'reg_lambda': 0.4964566404520624}. Best is trial 31 with value: 0.9857134302806969.\n",
      "[model_tuning_step] 2025-10-22 23:21:41 [INFO] src.Model_Tuner: Best Score: 0.9857\n",
      "[model_tuning_step] \u001b[1;35mBest Score: 0.9857\u001b[0m\n",
      "[model_tuning_step] 2025-10-22 23:21:41 [INFO] src.Model_Tuner: Best Params: {'max_depth': 8, 'learning_rate': 0.1311585671638698, 'n_estimators': 1636, 'subsample': 0.6329556767173284, 'colsample_bytree': 0.9650889366501909, 'gamma': 0.05242229446014293, 'min_child_weight': 8, 'reg_alpha': 0.09675251466272686, 'reg_lambda': 0.2488509824055381}\n",
      "[model_tuning_step] \u001b[1;35mBest Params: {'max_depth': 8, 'learning_rate': 0.1311585671638698, 'n_estimators': 1636, 'subsample': 0.6329556767173284, 'colsample_bytree': 0.9650889366501909, 'gamma': 0.05242229446014293, 'min_child_weight': 8, 'reg_alpha': 0.09675251466272686, 'reg_lambda': 0.2488509824055381}\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[1;36mmodel_tuning_step\u001b[1;35m has finished in \u001b[0m\u001b[1;36m4m43s\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mPipeline run has finished in \u001b[0m\u001b[1;36m4m43s\u001b[1;35m.\u001b[0m\n",
      "2025-10-22 23:21:49 [INFO] __main__: Model tuning done: XGBClassifier - Final score: 0.985713\n",
      "\u001b[1;35mModel tuning done: XGBClassifier - Final score: 0.985713\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# === Step 6: Model Tuning ===\n",
    "logger.info(\"Tuning best model...\")\n",
    "tuned_model, best_params, best_score = model_tuning_step(best_model_name=best_model_name,best_model = best_model,X_train=X_train,y_train=y_train)\n",
    "logger.info(f\"Model tuning done: {tuned_model.__class__.__name__} - Final score: {best_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bd138f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_dir = os.path.join(os.getcwd(), \"artifacts\")\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0792d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-22 23:23:21 [INFO] __main__: 💾 Model saved at /Users/hanli/cost_ml_202509/Cost_Minimasation_ML/artifacts/best_model_20251022_232321.pkl\n",
      "\u001b[1;35m💾 Model saved at /Users/hanli/cost_ml_202509/Cost_Minimasation_ML/artifacts/best_model_20251022_232321.pkl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = os.path.join(artifacts_dir, f\"best_model_{timestamp}.pkl\")\n",
    "joblib.dump(tuned_model, model_path)\n",
    "logger.info(f\"💾 Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effec230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictive-ml (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
